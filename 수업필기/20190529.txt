오전 10:07 2019-05-29

p29
여기있는 영상 처리에 대해 차례로 배움

p30
RGB 포맷을 YUV로 변환을 시킴 <-- 정보의 양을 줄이기 위해서
# RGB : 색의 3원색
YUV 
 Y(luma):명도(밝고 어두움) 
 U/V(chrominance):채도 
--> 표준으로 쓰임. 
사람의 눈의 특성에 집중. 사람은 채도보다 명도에 더 민감함.
그래서 Y에 보다 많은 정보를 할당 --> YUV-844모델이 가장 많이 쓰임.

아래 행렬은 RGB와 YUV의 관계를 나타냄

요즘은 거의다 HDMI케이블로 바껴서 아래 사진을 보기가 힘든데, 
여전히 오디오 뒤쪽을 보면 사진처럼 되어있음.
디지털 방식 두가지 - optical, coaxial
아날로그 - R, L
video 

p32
카메라 좋아하는 사람은 이걸 앎
4:4:4 4:2:2 뭐 이런건 영상 용어임

**p33
영상압축기법 이 세가지는 기억하자.
Redundancy를 줄여야하는데, 
Spatial, Temporal, Entropy 세가지가 있음.
Redundancy는 불필요하게 중복된 정보임.
이미지에 한 점이 있으면 그 점의 주변의 색깔은 점과 비슷한 경우가 많음.
그래서 이걸 개별적으로 보지말고 묶어서 보자.
공간적측면에서 보자는거.

p34
아 순간 딴생각함.
Differential Encdoing이라는 걸 했었잖아?
157 155 156 158 150 이 정보를 
157 -2 +1 +2 -8 로 정보를 줄여서 표현하는 것이 가능했잖아

마찬가지로, 
이미지를 있는그대로 해석하지 않고, 이미지의 대표값(예를들어 평균) 그안의 개별적인 차이를 분리하는거.
그니까 공간에서 비슷한가 아닌가로 분리하는거. 공간적 압축.
그림에서 왼쪽을 공간주파수가 낮다.

*DCT: 눈으로 보이는 화소값을 spatial frequency로 바꾸는거.

p35
FFT와 유사하게 spatial frequency domain으로 바꿔야함

p36
신호가 있을때 신호는 다양한 frequency들의 합인데,
low frequency가 전체 신호에서 가장 중요함.
빨리변하는 것들은 상대적으로 중요도가 떨어지기 때문에 
더 중요한 것에 더 많은 정보를 할당해야함. 
그래서 앞에 배치, 
DCT로 변환하게 되면 각각의 숫자가 나타내는건 frequency(대표하는 값)
(변환전에는 각각 픽셀을 의미했지만,)

Quantization : 값을 하나의 레벨로 부여하는 거. 주로 나누기로 많이 처리함.
예) 기존엔 8비트로 0~255까지 표현할 수 있는데, 
	7비트로 표현하고 싶다? 2로 나누면 됨.
	6비트 ? 4로 나눔.
Quantiazation Table안의 값이 작으면 작을수록 더 섬세함. 
아까 low가 더 중요하댔으니 low는 비교적 작은 값으로 나눠주고, 
뒤에는 비교적 큰 값으로 나눠줌

pixel -> DCT -> Quantization 의 결과로 Quantized Coefficient이 됨 
Quantized Coefficient를 다시 돌리면 압축된 영상이 됨.
이게 jpack? 임

이 결과를 지그재그로 스캐닝함. (더 중요한 걸 먼저 스캐닝)

RLE : Run Length Encoding
ex) 1100120000000
--> (1:2) (0:2) (1:1) (2:1) (0:)
RLE는 항상 효율적이지 않지만 뒤쪽이 다 0인경우와 같을때 효율적임.

p37
spatail -> Temporal 오타임
# FPS : Frame per Second
영상에서는 시간적 중복이 많이 일어남.

p38
여기도 오타(spatial -> temporal)
1. Motion Estimation 
	현재 이것을 표현하고 싶은데 과거에 있었는지 찾아냄. 
2. Motion Compensation 
	어딨었는지, 어디로 이동했는지 벡터를 찾음.

p40
Differential Encoding의 문제점
하나가 틀리면 뒤에도 다틀림.

시간적 압축의 기본 개념은 이전 프레임과의 차이만을 encoding하는것인데,
에러가 있을 수 있으므로 무한히 반복할 수가 없음.

그래서 나온게, Group of Pictures.
I-frame
	Interframe compression이 없음
	다른 녀석들과 비교하지 않는 기준임.
P-frame
	Interframe compression이 있음.
	Previous- 앞에만 비교
B-frame
	Interframe compression이 없음
	Bidirection- 앞 뒤를 다 본다는 거

결과적으로 정보의 양을 따지면,
하나의 I부터 그다음 I가 오기 전까지를 한 group이라고하는데
그 그룹내에서 B의 정보가 가장 많고, I의 정보의 양이 가장 없음

p41
Group of Pictures가 중요한 이유.
그림을 이해해볼것.
잘 보면 프레임의 재생순서가 다름.
B는 앞뒤 모두가 필요함. 그래서 뒤의 정보 뒤에 나와야함
ex) 6은 앞인 5와 뒤인 9가 모두 필요. 그래서 9뒤로 가야함.
앞으로나 뒤로가기 할때 프레임 단위로 잘 안짤리는 경우가 있는데 이거때문임.

쉬는시간.
공간적 압축 : 블럭단위로 표현
시간적 압축 : 앞뒤영상의 차이를 이용

이제 마지막, Entropy
p42
예) 오늘 5/29 부산 25도 라는 정보와 부산 -5도가 있다면,
어느게 더 큰 정보인가?
후자. 왜냐면 5월에 영하의 온도는 매우 이례적으로 중요한 정보임.
전자는 이미 알고있는 정보일 확률이 커서 그냥저냥한 정보임.

따라서 Information measure는 그것이 일어날 확률에 반비례한다.

p43
60프로일떄 log(1/0.6)
15프로일떄 log(1/0.15)
이 중 정보가 더 많은건? 15프로.

정보를 더 많이 갖고있다는 것은 잘 일어나지 않는다는 걸 의미.

그래서 frequency가 주어졌을 때 Entropy를 구하라고 하면,
(확률 * Information measure)의 합을 구하면 됨

예를 들어 확률이 모두 0.25라면 entropy == 2.
실제 2비트만큼의 information을 가지고있음을 의미

근데, 1.6이라면 실제 2비트보다 작은 정보를 가지고 있음.

ex) 00 01 00 01 10 11 00 10
--> 0 100 0 100 11 101 0 11
코드의 특성상 그대로 복원도 가능. 
전체 60퍼센트가 1비트로 표현이 됨.
20 퍼센트는 2비트로 표현됨.
나머지 20퍼센트는 3비트로 표현이 됨.
--> 0.6*1 + 0.2*2 + 0.2*3 == 1.6
결론 : 아무런 information loss없이 정보의 양이 줄이는 것이 가능.

ex) 00 - 0.5 / 01 - 0.125 / 10 - 0.25 / 11 - 0.125 일경우?
Entropy == 1.75

결론 : optimal 코딩임. 이부분 잘 못들음.

Huffman Coding, Lempel-Zip Coding : 여기에 다 entropy coding이 들어가있음



